{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUR8bNFEwLUW",
        "outputId": "57476149-93c8-47b4-a31c-779a889cc6f9",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain[google-genai] in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (6.0.2)\n",
            "Collecting langchain-google-genai (from langchain[google-genai])\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain[google-genai]) (3.2.3)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai->langchain[google-genai])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai->langchain[google-genai])\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (5.29.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (3.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (4.9.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (0.6.1)\n",
            "Downloading langchain_google_genai-2.1.7-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "107e169704064ecab0cda33c570ab09c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \"langchain[google-genai]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-huggingface\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tua7wH03Qsy",
        "outputId": "b73487dc-35b5-4f6a-ca40-5473d304a489",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv(r\"/content/drive/MyDrive/.env\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD7RxAQZvdou",
        "outputId": "8b5b4398-6d82-4ad9-a51b-e29b667e39d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "id": "WQJONHz2z7aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_resp = model.invoke(\"who are you, which model\")\n",
        "model_resp.to_json()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eYaA78e0Esd",
        "outputId": "bfbcbb89-fb66-4c4d-9e8e-2e40eba67cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lc': 1,\n",
              " 'type': 'constructor',\n",
              " 'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
              " 'kwargs': {'content': 'I am a large language model, trained by Google.',\n",
              "  'response_metadata': {'prompt_feedback': {'block_reason': 0,\n",
              "    'safety_ratings': []},\n",
              "   'finish_reason': 'STOP',\n",
              "   'model_name': 'gemini-2.0-flash',\n",
              "   'safety_ratings': []},\n",
              "  'type': 'ai',\n",
              "  'id': 'run--7151cf61-95ba-4781-9f21-59542a9528c4-0',\n",
              "  'usage_metadata': {'input_tokens': 6,\n",
              "   'output_tokens': 12,\n",
              "   'total_tokens': 18,\n",
              "   'input_token_details': {'cache_read': 0}},\n",
              "  'tool_calls': [],\n",
              "  'invalid_tool_calls': []}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4daoKuQqBrX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"Translate the following from English into hindi.\"),\n",
        "    HumanMessage(\"hi!\"),\n",
        "]\n",
        "\n",
        "resp = model.invoke(messages)\n",
        "resp.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOZv-3UWBr1P",
        "outputId": "b1008af1-35b8-4bad-ce6e-c429330b5da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'हाय!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "cPtXzrQUBzEP",
        "outputId": "64691476-bb1c-436d-e5d8-727fb81831d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
              "\n",
              "AIMessage is returned from a chat model as a response to a prompt.\n",
              "\n",
              "This message represents the output of the model and consists of both\n",
              "the raw output as returned by the model together standardized fields\n",
              "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 151);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bzt_8cjfB2l_",
        "outputId": "299b64e1-bed0-48bc-a385-d0a5ba140b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__abstractmethods__',\n",
              " '__add__',\n",
              " '__annotations__',\n",
              " '__class__',\n",
              " '__class_getitem__',\n",
              " '__class_vars__',\n",
              " '__copy__',\n",
              " '__deepcopy__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__fields__',\n",
              " '__fields_set__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__get_pydantic_core_schema__',\n",
              " '__get_pydantic_json_schema__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pretty__',\n",
              " '__private_attributes__',\n",
              " '__pydantic_complete__',\n",
              " '__pydantic_computed_fields__',\n",
              " '__pydantic_core_schema__',\n",
              " '__pydantic_custom_init__',\n",
              " '__pydantic_decorators__',\n",
              " '__pydantic_extra__',\n",
              " '__pydantic_fields__',\n",
              " '__pydantic_fields_set__',\n",
              " '__pydantic_generic_metadata__',\n",
              " '__pydantic_init_subclass__',\n",
              " '__pydantic_parent_namespace__',\n",
              " '__pydantic_post_init__',\n",
              " '__pydantic_private__',\n",
              " '__pydantic_root_model__',\n",
              " '__pydantic_serializer__',\n",
              " '__pydantic_setattr_handlers__',\n",
              " '__pydantic_validator__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__replace__',\n",
              " '__repr__',\n",
              " '__repr_args__',\n",
              " '__repr_name__',\n",
              " '__repr_recursion__',\n",
              " '__repr_str__',\n",
              " '__rich_repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__signature__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abc_impl',\n",
              " '_backwards_compat_tool_calls',\n",
              " '_calculate_keys',\n",
              " '_copy_and_set_values',\n",
              " '_get_value',\n",
              " '_iter',\n",
              " '_setattr_handler',\n",
              " 'additional_kwargs',\n",
              " 'construct',\n",
              " 'content',\n",
              " 'copy',\n",
              " 'dict',\n",
              " 'example',\n",
              " 'from_orm',\n",
              " 'get_lc_namespace',\n",
              " 'id',\n",
              " 'invalid_tool_calls',\n",
              " 'is_lc_serializable',\n",
              " 'json',\n",
              " 'lc_attributes',\n",
              " 'lc_id',\n",
              " 'lc_secrets',\n",
              " 'model_computed_fields',\n",
              " 'model_config',\n",
              " 'model_construct',\n",
              " 'model_copy',\n",
              " 'model_dump',\n",
              " 'model_dump_json',\n",
              " 'model_extra',\n",
              " 'model_fields',\n",
              " 'model_fields_set',\n",
              " 'model_json_schema',\n",
              " 'model_parametrized_name',\n",
              " 'model_post_init',\n",
              " 'model_rebuild',\n",
              " 'model_validate',\n",
              " 'model_validate_json',\n",
              " 'model_validate_strings',\n",
              " 'name',\n",
              " 'parse_file',\n",
              " 'parse_obj',\n",
              " 'parse_raw',\n",
              " 'pretty_print',\n",
              " 'pretty_repr',\n",
              " 'response_metadata',\n",
              " 'schema',\n",
              " 'schema_json',\n",
              " 'text',\n",
              " 'to_json',\n",
              " 'to_json_not_implemented',\n",
              " 'tool_calls',\n",
              " 'type',\n",
              " 'update_forward_refs',\n",
              " 'usage_metadata',\n",
              " 'validate']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resp.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEnp8ZXwB8j7",
        "outputId": "d8303541-f1a1-4b34-c9a2-6639b78763b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "हाय!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in model.stream(messages):\n",
        "    print(token.content, end=\"<..>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tNuTWsJCOdm",
        "outputId": "a7d1e636-11b9-439e-9c8a-732af7834739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "हाय<..>! (Haay!) - This is the most common and direct translation.\n",
            "\n",
            "नमस्ते<..>! (Namaste!) - This is a more formal and respectful greeting.\n",
            "\n",
            "हे<..>ल्लो! (Hello!) - You can also use the English word \"Hello\" as it is widely understood in India.\n",
            "<..>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming with async"
      ],
      "metadata": {
        "id": "a0Xdb6u3LWHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize the chat model\n",
        "chatmodel = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "\n",
        "# Use async directly at the top level in Jupyter, ignore the error or just wrap the api call in the async function and just await that function (without a new async loop)\n",
        "async for chunk in chatmodel.astream(\"Write me a 1 verse song about goldfish on the moon\"):\n",
        "    print(chunk.content, end=\"|\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6USit7dzCOT-",
        "outputId": "143712d1-aa72-4b81-b0b9-c4709bc9f602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(|Verse 1)\n",
            "In a rocket built of seaweed and of dreams,\n",
            "A| goldfish swam to lunar, silver gleams.\n",
            "He puffed a bubble, watched| the Earth below,\n",
            "A tiny finned astronaut, putting on a show.\n",
            "No water needed, just a moonbeam's kiss,\n",
            "A goldfish on| the moon, pure cosmic bliss.\n",
            "|"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR"
      ],
      "metadata": {
        "id": "il6E2nWvK2qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# nest_asyncio.apply()  # Patch the running event loop\n",
        "\n",
        "async def main():\n",
        "    chatmodel = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "    async for chunk in chatmodel.astream(\"Write me a 1 verse song about goldfish on the moon\"):\n",
        "        print(chunk.content, end=\"|\", flush=True)\n",
        "\n",
        "# Works now in Colab/Jupyter\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkJKPIiTKqtp",
        "outputId": "900d2956-a0cc-469a-a2fe-b1ba5057605c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In| a lunar fishbowl, silver bright,\n",
            "Swim goldfish, weightless, in pale| moonlight.\n",
            "They bubble no words, just silent glee,\n",
            "Dancing among the stars, wild| and free.\n",
            "|"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatPromptTemplate"
      ],
      "metadata": {
        "id": "OWH32B8lP5sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English into {language}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")\n",
        "# prompt_template = ChatPromptTemplate.from_messages(\n",
        "#     [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        "# )\n",
        "prompt = prompt_template.invoke({\"language\": \"hindi\", \"text\": \"hi!\"})\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWH-n6YvLZtq",
        "outputId": "1e35d01e-c52e-4f21-a864-2150a1f93371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into hindi', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b9Nbu_gyLZq_",
        "outputId": "da0ce27d-addb-4e1d-eb21-a04627aca478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__abstractmethods__',\n",
              " '__annotations__',\n",
              " '__class__',\n",
              " '__class_getitem__',\n",
              " '__class_vars__',\n",
              " '__copy__',\n",
              " '__deepcopy__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__fields__',\n",
              " '__fields_set__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__get_pydantic_core_schema__',\n",
              " '__get_pydantic_json_schema__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pretty__',\n",
              " '__private_attributes__',\n",
              " '__pydantic_complete__',\n",
              " '__pydantic_computed_fields__',\n",
              " '__pydantic_core_schema__',\n",
              " '__pydantic_custom_init__',\n",
              " '__pydantic_decorators__',\n",
              " '__pydantic_extra__',\n",
              " '__pydantic_fields__',\n",
              " '__pydantic_fields_set__',\n",
              " '__pydantic_generic_metadata__',\n",
              " '__pydantic_init_subclass__',\n",
              " '__pydantic_parent_namespace__',\n",
              " '__pydantic_post_init__',\n",
              " '__pydantic_private__',\n",
              " '__pydantic_root_model__',\n",
              " '__pydantic_serializer__',\n",
              " '__pydantic_setattr_handlers__',\n",
              " '__pydantic_validator__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__replace__',\n",
              " '__repr__',\n",
              " '__repr_args__',\n",
              " '__repr_name__',\n",
              " '__repr_recursion__',\n",
              " '__repr_str__',\n",
              " '__rich_repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__signature__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abc_impl',\n",
              " '_calculate_keys',\n",
              " '_copy_and_set_values',\n",
              " '_get_value',\n",
              " '_iter',\n",
              " '_setattr_handler',\n",
              " 'construct',\n",
              " 'copy',\n",
              " 'dict',\n",
              " 'from_orm',\n",
              " 'get_lc_namespace',\n",
              " 'is_lc_serializable',\n",
              " 'json',\n",
              " 'lc_attributes',\n",
              " 'lc_id',\n",
              " 'lc_secrets',\n",
              " 'messages',\n",
              " 'model_computed_fields',\n",
              " 'model_config',\n",
              " 'model_construct',\n",
              " 'model_copy',\n",
              " 'model_dump',\n",
              " 'model_dump_json',\n",
              " 'model_extra',\n",
              " 'model_fields',\n",
              " 'model_fields_set',\n",
              " 'model_json_schema',\n",
              " 'model_parametrized_name',\n",
              " 'model_post_init',\n",
              " 'model_rebuild',\n",
              " 'model_validate',\n",
              " 'model_validate_json',\n",
              " 'model_validate_strings',\n",
              " 'parse_file',\n",
              " 'parse_obj',\n",
              " 'parse_raw',\n",
              " 'schema',\n",
              " 'schema_json',\n",
              " 'to_json',\n",
              " 'to_json_not_implemented',\n",
              " 'to_messages',\n",
              " 'to_string',\n",
              " 'update_forward_refs',\n",
              " 'validate']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU7aUZYdLZoG",
        "outputId": "53aa4fc5-4906-4693-e533-4633fbc7608d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following from English into hindi', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltfevCFXLZlb",
        "outputId": "ec92f265-0fc9-4505-c20d-e3a2da23e464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "नमस्ते! (Namaste!)\n",
            "\n",
            "Or, more informally:\n",
            "\n",
            "हेलो! (Hello!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install google-generativeai\n"
      ],
      "metadata": {
        "id": "lQA1OcZCTECq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8d98c8b2-cbd6-4787-d6f3-a73155361bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.175.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.6.15)\n",
            "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-ai-generativelanguage\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.18\n",
            "    Uninstalling google-ai-generativelanguage-0.6.18:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.18\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.1.7 requires google-ai-generativelanguage<0.7.0,>=0.6.18, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.6.15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "c1c8bcb18580489092603ba3eb3c2f15"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import time\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "# Upload file\n",
        "file = client.files.upload(file=\"/content/drive/MyDrive/lanchain-practice/hsm.pdf\")\n",
        "while file.state.name == 'PROCESSING':\n",
        "    time.sleep(2)\n",
        "    file = client.files.get(name=file.name)\n",
        "\n",
        "# Create cache\n",
        "model = 'gemini-2.0-flash'\n",
        "cache = client.caches.create(\n",
        "    model=model,\n",
        "    config=types.CreateCachedContentConfig(\n",
        "        display_name='Cached Content',\n",
        "        system_instruction=(\n",
        "            \"You are an expert content analyzer, and your job is to answer the user's query based on the file you have access to.\"\n",
        "        ),\n",
        "        contents=[file],\n",
        "        ttl=\"300s\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# Query with LangChain\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=model,\n",
        "    cached_content=cache.name,\n",
        ")\n",
        "message = HumanMessage(content=\"Summarize the main points of the content.\")\n",
        "airesponse = llm.invoke([message])"
      ],
      "metadata": {
        "id": "T7ApKYZrTD-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3JCr5thUFyU",
        "outputId": "d529652a-531a-4d52-badc-2f779746801d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CachedContent(\n",
              "  create_time=datetime.datetime(2025, 7, 10, 16, 25, 50, 82613, tzinfo=TzInfo(UTC)),\n",
              "  display_name='Cached Content',\n",
              "  expire_time=datetime.datetime(2025, 7, 10, 16, 30, 49, 613098, tzinfo=TzInfo(UTC)),\n",
              "  model='models/gemini-2.0-flash',\n",
              "  name='cachedContents/arpxrpn2dr6nhxmtl0lxqkmguw4mxetebnn337re',\n",
              "  update_time=datetime.datetime(2025, 7, 10, 16, 25, 50, 82613, tzinfo=TzInfo(UTC)),\n",
              "  usage_metadata=CachedContentUsageMetadata(\n",
              "    total_token_count=8285\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(client)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5S7iL7pTA_pw",
        "outputId": "30a34e8a-28cb-4c68-f0be-4d13c13d3e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_aio',\n",
              " '_api_client',\n",
              " '_batches',\n",
              " '_caches',\n",
              " '_debug_config',\n",
              " '_files',\n",
              " '_get_api_client',\n",
              " '_models',\n",
              " '_operations',\n",
              " '_tokens',\n",
              " '_tunings',\n",
              " 'aio',\n",
              " 'auth_tokens',\n",
              " 'batches',\n",
              " 'caches',\n",
              " 'chats',\n",
              " 'files',\n",
              " 'models',\n",
              " 'operations',\n",
              " 'tunings',\n",
              " 'vertexai']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.caches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0VXDeohBDlX",
        "outputId": "3d5cb9ad-1008-43a5-efb8-e9c39fb0218a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<google.genai.caches.Caches at 0x7f8595d3d5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "osGiNj_jBYzC",
        "outputId": "abbfb306-aa0b-4ec1-f9c5-3b475e6ce8a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'files/6q22rhyy1spc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "client.caches.get(name=\"files/rvmpfysdfu0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEYC9jeeBklC",
        "outputId": "9146a11e-9361-42de-c34e-47c1189f657e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CachedContent(\n",
              "  create_time=datetime.datetime(2025, 7, 9, 8, 30, 28, 100941, tzinfo=TzInfo(UTC)),\n",
              "  name='files/rvmpfysdfu0m',\n",
              "  update_time=datetime.datetime(2025, 7, 9, 8, 30, 28, 100941, tzinfo=TzInfo(UTC))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file.state.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U64EFXRRAl-O",
        "outputId": "935269d6-0095-4098-8be2-55c6ce333317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ACTIVE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(airesponse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "HKDLslIKTD7N",
        "outputId": "99c3c662-16a7-4292-f7df-c95983572de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
              "\n",
              "AIMessage is returned from a chat model as a response to a prompt.\n",
              "\n",
              "This message represents the output of the model and consists of both\n",
              "the raw output as returned by the model together standardized fields\n",
              "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 151);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "airesponse.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "3rYpWP5ksWzs",
        "outputId": "58b7f964-9684-420c-8985-4cbd943c7b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here\\'s a summary of the document\\'s main points:\\n\\n*   **Hardware Security Modules (HSMs) as the Gold Standard:** The document begins by defining HSMs as physical devices providing a secure environment for cryptographic key management, emphasizing their role as a hardware Root of Trust (RoT). It highlights their internal architecture (secure processors, tamper-resistant storage, random number generators) and physical/logical defenses.\\n\\n*   **Demystifying \"Crypto Tokens\":** It clarifies the differences between HSMs and other technologies often referred to as \"crypto tokens,\" distinguishing between portable cryptographic tokens (USB tokens, smart cards) and blockchain crypto-tokens. It emphasizes that these are fundamentally different from HSMs in terms of purpose, architecture, and performance.\\n\\n*   **Blueprint for a Cloud-Native SoftHSM Service:** The document then shifts to the concept of a Software HSM (SoftHSM) as a cloud-native alternative, aiming to provide similar functionality without the physical appliance. It discusses the architecture of existing SoftHSMs like OpenDNSSEC\\'s, pointing out their inherent security limitations due to reliance on general-purpose operating systems.\\n\\n*   **Forging a Software Root of Trust with Trusted Execution Environments (TEEs):** It proposes using Trusted Execution Environments (TEEs), like Intel SGX or AMD SEV-SNP, to create a more secure SoftHSM by providing a hardware-enforced boundary in software.\\n\\n*   **Designing for a Multi-Tenant Cloud Environment:** The document addresses the challenges of offering a SoftHSM as a public cloud service, outlining architectural patterns for multi-tenancy and emphasizing the importance of cryptographic isolation and robust identity/access management.\\n\\n*   **Scalability and High Availability:** It emphasizes the need for a SoftHSM service to be scalable and resilient, advocating for stateless architecture and leveraging cloud-native services.\\n\\n*   **Implementation Strategy:** The document discusses the \"build vs. buy\" dilemma for components, recommending a hybrid approach that combines open standards with commercial, FIPS-validated cryptographic SDKs.\\n\\n*   **API and Standards Compliance:** It stresses the importance of adhering to established standards, particularly the PKCS#11 API, for interoperability.\\n\\n*   **Target Use Cases and Required Functionality:** It identifies key use cases for a SoftHSM service, including general-purpose key management, TLS/SSL offloading, digital signing, and blockchain/digital asset custody.\\n\\n*   **Governance, Risk, Compliance, and Best Practices:** The document concludes by addressing the non-technical aspects of running a cryptographic service, emphasizing the need for legal compliance, operational security, key management best practices, and insider threat mitigation. It also highlights the challenges of key management as a service (KaaS) and the importance of customer education.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent using langgraph"
      ],
      "metadata": {
        "id": "h8IdRl1vzYxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite"
      ],
      "metadata": {
        "id": "lk_sCcrrGwum",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5996cd37-f9d7-43f5-8393-86c175dd1845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.5.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting langchain-tavily\n",
            "  Downloading langchain_tavily-0.2.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting langgraph-checkpoint-sqlite\n",
            "  Downloading langgraph_checkpoint_sqlite-2.0.10-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.68)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.72-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (3.11.15)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (0.3.26)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (2.32.3)\n",
            "Collecting aiosqlite>=0.20 (from langgraph-checkpoint-sqlite)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting sqlite-vec>=0.1.6 (from langgraph-checkpoint-sqlite)\n",
            "  Downloading sqlite_vec-0.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux1_x86_64.whl.metadata (198 bytes)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.20.1)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite>=0.20->langgraph-checkpoint-sqlite) (4.14.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (0.4.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2025.6.15)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.23.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (3.2.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.5.2-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_tavily-0.2.7-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_checkpoint_sqlite-2.0.10-py3-none-any.whl (30 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.72-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlite_vec-0.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux1_x86_64.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlite-vec, ormsgpack, aiosqlite, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph-checkpoint-sqlite, langgraph, langchain-tavily\n",
            "Successfully installed aiosqlite-0.21.0 langchain-tavily-0.2.7 langgraph-0.5.2 langgraph-checkpoint-2.1.0 langgraph-checkpoint-sqlite-2.0.10 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.72 ormsgpack-1.10.0 sqlite-vec-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "from langchain.tools import tool"
      ],
      "metadata": {
        "id": "8KfVjPVWGwr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
        "  os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter API key for TAVILY_API_KEY: \")"
      ],
      "metadata": {
        "id": "5WSgB1PG0tBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = TavilySearch(max_results=2)\n",
        "search_results = search.invoke(\"What is the weather in New Delhi\")\n",
        "print(search_results)\n",
        "# If we want, we can create other tools.\n",
        "# Once we have all the tools we want, we can put them in a list that we will reference later.\n",
        "# from pydantic import BaseModel\n",
        "\n",
        "# class SearchInput(BaseModel):\n",
        "#     query: str\n",
        "\n",
        "@tool\n",
        "def search(query) -> str:\n",
        "    \"\"\"Search the web for recent information about anything you don't know.\"\"\"\n",
        "    print(f\"In the search tool now: params i got - > query:{query}\")\n",
        "    result = search.invoke(input)\n",
        "    return str(result)\n",
        "\n",
        "tools = [search]"
      ],
      "metadata": {
        "id": "1sFXnvYSGwpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237b43a1-c151-41d3-9a62-0e6be463edaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'What is the weather in New Delhi', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Weather in New Delhi', 'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'New Delhi', 'region': 'Delhi', 'country': 'India', 'lat': 28.6, 'lon': 77.2, 'tz_id': 'Asia/Kolkata', 'localtime_epoch': 1752164780, 'localtime': '2025-07-10 21:56'}, 'current': {'last_updated_epoch': 1752164100, 'last_updated': '2025-07-10 21:45', 'temp_c': 28.0, 'temp_f': 82.4, 'is_day': 0, 'condition': {'text': 'Mist', 'icon': '//cdn.weatherapi.com/weather/64x64/night/143.png', 'code': 1030}, 'wind_mph': 5.8, 'wind_kph': 9.4, 'wind_degree': 91, 'wind_dir': 'E', 'pressure_mb': 1003.0, 'pressure_in': 29.62, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 84, 'cloud': 75, 'feelslike_c': 30.6, 'feelslike_f': 87.1, 'windchill_c': 29.4, 'windchill_f': 84.8, 'heatindex_c': 33.2, 'heatindex_f': 91.7, 'dewpoint_c': 22.8, 'dewpoint_f': 73.0, 'vis_km': 4.5, 'vis_miles': 2.0, 'uv': 0.0, 'gust_mph': 7.5, 'gust_kph': 12.1}}\", 'score': 0.9114088, 'raw_content': None}, {'url': 'https://world-weather.info/forecast/india/delhi/july-2025/', 'title': 'Weather in Delhi in July 2025', 'content': \"Weather in Delhi in July 2025 (Union Territory of Delhi) - Detailed Weather Forecast for a Month Weather World Weather in Delhi Weather in Delhi in July 2025 Delhi Weather Forecast for July 2025, is based on previous years' statistical data. +100°+90° +99°+88° +97°+88° +97°+88° +95°+86° +93°+84° +97°+88° +97°+88° +95°+86° +97°+88° +95°+84° +93°+86° +95°+88° +95°+86° +95°+86° +95°+88° +93°+84° +93°+84° +95°+86° +95°+84° +93°+84° +93°+86° +93°+84° +95°+86° +93°+84° +91°+82° +91°+84° +93°+84° +91°+82° +93°+84° +93°+84° Extended weather forecast in Delhi HourlyWeek10-Day14-Day30-DayYear Weather in large and nearby cities Weather in New Delhi+91° Panipat+93° Muzaffarnagar+95° Karnāl+95° Aligarh+95° Alwar+95° Mathura+95° Sahāranpur+93° Morādābād+95° Rohtak+93° Meerut+93° Okhla Industrial Development Area+88° Nāngloi Jāt+88° Ghaziabad+91° Noida+93° Faridabad+88° Gurgaon+88° Sonīpat+91° world's temperature today day day Weather forecast on your site Install Delhi+88°\", 'score': 0.8934678, 'raw_content': None}], 'response_time': 2.47}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"langchain[google-genai]\""
      ],
      "metadata": {
        "id": "NzQQRC55XAIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "id": "NnkA8js3XG3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hi!\"\n",
        "response = model.invoke([{\"role\": \"user\", \"content\": query}])\n",
        "response.text()"
      ],
      "metadata": {
        "id": "HSehtt_KGwmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_plus_tools = model.bind_tools(tools)\n",
        "\n",
        "query = \"What is the weather in New Delhi?\"\n",
        "response = model_plus_tools.invoke([{\"role\": \"user\", \"content\": query}])\n",
        "\n",
        "print(f\"Message content: {response.text()}\\n\")\n",
        "# here there is no response but as we can see that the tool is being called.\n",
        "print(f\"Tool calls: {response.tool_calls}\")"
      ],
      "metadata": {
        "id": "rV3OE9X1GwiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.tool_calls[0]"
      ],
      "metadata": {
        "id": "EkLso4_fd21V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H1AUv5aud8YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the agent using Langrapgh now"
      ],
      "metadata": {
        "id": "fgeGPexgbMHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Note that we are passing in the model, not model_with_tools. That is because create_react_agent will call .bind_tools for us under the hood.\n",
        "react_agent_executor = create_react_agent(\n",
        "    model=model,\n",
        "    tools=tools,\n",
        "    prompt=\"You are a helpful assistant. Use tools when needed.\"\n",
        ")"
      ],
      "metadata": {
        "id": "oYmX98hxbR2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now using a relevant input prompt\n",
        "input_message = {\"role\": \"user\", \"content\": \"What is the current weather in New Delhi?\"}\n",
        "response = react_agent_executor.invoke({\"messages\": [input_message]})\n",
        "\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()\n",
        "\n",
        "response"
      ],
      "metadata": {
        "id": "Yom0Pw8zbufx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26PooOTljqcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLEjIviFjqZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WORKING CODE !!!\n",
        "\n",
        "# 1️⃣ Imports & setup\n",
        "import os, getpass\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# 2️⃣ API keys\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Google Gemini API key: \")\n",
        "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key: \")\n",
        "\n",
        "# 3️⃣ Model initialization\n",
        "model = init_chat_model(\n",
        "    \"gemini-2.0-flash\",\n",
        "    model_provider=\"google_genai\"\n",
        ")\n",
        "\n",
        "# 4️⃣ TavilySearch instance\n",
        "search_instance = TavilySearch(max_results=2)\n",
        "\n",
        "# 5️⃣ Tool definition (match `query` signature)\n",
        "@tool\n",
        "def search(query: str) -> str:\n",
        "    \"\"\"Search the web for `query`.\"\"\"\n",
        "    result = search_instance.invoke(query)\n",
        "    return str(result)\n",
        "\n",
        "tools = [search]\n",
        "\n",
        "# 6️⃣ Create ReAct agent (auto .bind_tools under the hood)\n",
        "react_agent = create_react_agent(model=model, tools=tools)\n",
        "\n",
        "# 7️⃣ Invoke and print\n",
        "resp = react_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What is the current weather in New Delhi?\"}]\n",
        "})\n",
        "print(resp[\"messages\"][-1].content)\n"
      ],
      "metadata": {
        "id": "A38TN2VrjqWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard libraries\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Import the LangChain components\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# Import Pydantic for structured inputs\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Import ReAct agent creation\n",
        "from langgraph.prebuilt import create_react_agent\n"
      ],
      "metadata": {
        "id": "EmUjJbATj6ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tool 1: search_web\n",
        "\n",
        "# Pydantic schema for search_web inputs\n",
        "class WebSearchInput(BaseModel):\n",
        "    query: str\n",
        "    num_results: int\n",
        "\n",
        "# Tool definition using Pydantic\n",
        "@tool\n",
        "def search_web(input_data: WebSearchInput) -> str:\n",
        "    \"\"\"\n",
        "    Search the web with a query and number of results.\n",
        "    \"\"\"\n",
        "    # Perform search\n",
        "    result = search_instance.invoke(input_data.query, k=input_data.num_results)\n",
        "    # Return stringified result\n",
        "    return str(result)\n"
      ],
      "metadata": {
        "id": "Btbm-t-xj6Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tool 2: search_weather\n",
        "\n",
        "# Pydantic schema for weather search inputs\n",
        "class WeatherSearchInput(BaseModel):\n",
        "    city: str\n",
        "    unit: str  # e.g., \"Celsius\" or \"Fahrenheit\"\n",
        "\n",
        "@tool\n",
        "def search_weather(input_data: WeatherSearchInput) -> str:\n",
        "    \"\"\"\n",
        "    Search weather information for a city.\n",
        "    \"\"\"\n",
        "    query = f\"current weather in {input_data.city} in {input_data.unit}\"\n",
        "    result = search_instance.invoke(query)\n",
        "    return str(result)\n"
      ],
      "metadata": {
        "id": "c8kpgldRlQ0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tool 3 : search_news\n",
        "\n",
        "# Pydantic schema for news search inputs\n",
        "class NewsSearchInput(BaseModel):\n",
        "    topic: str\n",
        "    region: str\n",
        "\n",
        "@tool\n",
        "def search_news(input_data: NewsSearchInput) -> str:\n",
        "    \"\"\"\n",
        "    Search recent news headlines for a topic in a specific region.\n",
        "    \"\"\"\n",
        "    query = f\"latest news about {input_data.topic} in {input_data.region}\"\n",
        "    result = search_instance.invoke(query)\n",
        "    return str(result)\n"
      ],
      "metadata": {
        "id": "2kDYfWNMlYRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Register Tools\n",
        "# All tools are collected here\n",
        "tools = [\n",
        "    search_web,\n",
        "    search_weather,\n",
        "    search_news\n",
        "]\n"
      ],
      "metadata": {
        "id": "wJN3R72nlaQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ReAct agent with the model and tools\n",
        "react_agent_2 = create_react_agent(\n",
        "    model=model,\n",
        "    tools=tools\n",
        ")\n"
      ],
      "metadata": {
        "id": "Mt1x4uOhlaNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "ZhnCqwUxm2xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User query to trigger one of the tools\n",
        "resp = react_agent_2.invoke({\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Give me the latest news and weather in India, new delhi\"}\n",
        "    ]\n",
        "}\n",
        "  )\n",
        "pprint(len(resp[\"messages\"]))\n",
        "\n",
        "print(resp[\"messages\"][-1].content)\n",
        "# pprint(resp[\"messages\"][-2].content)\n",
        "\n",
        "\n",
        "for message in resp[\"messages\"]:\n",
        "    message.pretty_print()"
      ],
      "metadata": {
        "id": "vKGR2PColaKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4QU3BInglaH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4KIUMIngQJiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TsWZOAO2QJhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j-m8fBrUQJd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xR-YbMAmQJbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard libraries\n",
        "import os\n",
        "import getpass\n",
        "from typing import TypedDict\n",
        "from pprint import pprint\n",
        "\n",
        "# Import the LangChain components\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Import Pydantic for structured inputs\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Import ReAct agent creation\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# API keys setup\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Google Gemini API key: \")\n",
        "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key: \")\n",
        "\n",
        "# Model initialization\n",
        "model = init_chat_model(\n",
        "    \"gemini-2.0-flash\",\n",
        "    model_provider=\"google_genai\"\n",
        ")\n",
        "\n",
        "# TavilySearch instance\n",
        "search_instance = TavilySearch(max_results=2)\n",
        "\n",
        "## Tool 1: search_web\n",
        "# Pydantic schema for search_web inputs\n",
        "class WebSearchInput(BaseModel):\n",
        "    query: str\n",
        "    num_results: int = 2  # Default value\n",
        "\n",
        "# Tool definition using Pydantic\n",
        "@tool\n",
        "def search_web(input_data: WebSearchInput) -> str:\n",
        "    \"\"\"\n",
        "    Search the web with a query and number of results.\n",
        "    \"\"\"\n",
        "    # Note: TavilySearch uses max_results parameter, not k\n",
        "    search_temp = TavilySearch(max_results=input_data.num_results)\n",
        "    result = search_temp.invoke(input_data.query)\n",
        "    return str(result)\n",
        "\n",
        "## Tool 2: search_weather\n",
        "# Pydantic schema for weather search inputs\n",
        "class WeatherSearchInput(BaseModel):\n",
        "    city: str\n",
        "    unit: str = \"Celsius\"  # Default value\n",
        "\n",
        "@tool\n",
        "def search_weather(input_data: WeatherSearchInput) -> str:\n",
        "    \"\"\"\n",
        "    Search weather information for a city.\n",
        "    \"\"\"\n",
        "    query = f\"current weather in {input_data.city} in {input_data.unit}\"\n",
        "    result = search_instance.invoke(query)\n",
        "    return str(result)\n",
        "\n",
        "## Tool 3: search_news\n",
        "# Pydantic schema for news search inputs\n",
        "class NewsSearchInput(BaseModel):\n",
        "    topic: str\n",
        "    region: str\n",
        "\n",
        "@tool\n",
        "def search_news(input_data: NewsSearchInput) -> str:\n",
        "    \"\"\"\n",
        "    Search recent news headlines for a topic in a specific region.\n",
        "    \"\"\"\n",
        "    query = f\"latest news about {input_data.topic} in {input_data.region}\"\n",
        "    result = search_instance.invoke(query)\n",
        "    return str(result)\n",
        "\n",
        "## Register Tools\n",
        "# All tools are collected here\n",
        "tools = [\n",
        "    search_web,\n",
        "    search_weather,\n",
        "    search_news\n",
        "]\n",
        "\n",
        "# Create ReAct agent with the model and tools\n",
        "react_agent = create_react_agent(\n",
        "    model=model,\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "# Test the basic ReAct agent\n",
        "print(\"=== Testing Basic ReAct Agent ===\")\n",
        "resp = react_agent.invoke({\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Give me the latest news and weather in India, New Delhi\"}\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(f\"Number of messages: {len(resp['messages'])}\")\n",
        "print(f\"Final response: {resp['messages'][-1].content}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FOkeQQ36QJX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81665d91-ce5e-4096-ac9f-f34e5df5811e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing Basic ReAct Agent ===\n",
            "Number of messages: 5\n",
            "Final response: The latest news from India is available from The Indian Express and NDTV.com. The weather in New Delhi is currently 28 degrees Celsius with mist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Define State for Custom Agent using TypedDict\n",
        "class AgentState(TypedDict):\n",
        "    user_input: str\n",
        "    response: str\n",
        "\n",
        "# ✅ Wrap ReAct agent as node\n",
        "def run_react_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"Run the ReAct agent and update state with response\"\"\"\n",
        "    # ReAct agent takes list of messages\n",
        "    result = react_agent.invoke({\n",
        "        \"messages\": [HumanMessage(content=state[\"user_input\"])]\n",
        "    })\n",
        "\n",
        "    # Update state with response\n",
        "    state[\"response\"] = result[\"messages\"][-1].content\n",
        "    return state\n",
        "\n",
        "# ✅ Create StateGraph\n",
        "def create_custom_agent():\n",
        "    \"\"\"Create a custom agent using StateGraph\"\"\"\n",
        "\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(AgentState)\n",
        "\n",
        "    # Add the ReAct agent node\n",
        "    workflow.add_node(\"react_agent\", run_react_agent)\n",
        "\n",
        "    # Define the flow\n",
        "    workflow.add_edge(START, \"react_agent\")\n",
        "    workflow.add_edge(\"react_agent\", END)\n",
        "\n",
        "    # Compile the graph\n",
        "    return workflow.compile()\n",
        "\n",
        "# ✅ Test the custom StateGraph agent\n",
        "print(\"\\n=== Testing Custom StateGraph Agent ===\")\n",
        "custom_agent = create_custom_agent()\n",
        "\n",
        "# Test with initial state\n",
        "initial_state = {\n",
        "    \"user_input\": \"What's the weather like in New Delhi today?\",\n",
        "    \"response\": \"\"\n",
        "}\n",
        "\n",
        "result = custom_agent.invoke(initial_state)\n",
        "print(f\"User Input: {result['user_input']}\")\n",
        "print(f\"Agent Response: {result['response']}\")\n",
        "\n",
        "# ✅ Another test with news query\n",
        "print(\"\\n=== Testing News Query ===\")\n",
        "news_state = {\n",
        "    \"user_input\": \"Give me the latest technology news in India\",\n",
        "    \"response\": \"\"\n",
        "}\n",
        "\n",
        "news_result = custom_agent.invoke(news_state)\n",
        "print(f\"User Input: {news_result['user_input']}\")\n",
        "print(f\"Agent Response: {news_result['response']}\")"
      ],
      "metadata": {
        "id": "lhG3vbnZQJVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2637e361-8419-46d3-e009-eac4b43a3889"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing Custom StateGraph Agent ===\n",
            "User Input: What's the weather like in New Delhi today?\n",
            "Agent Response: Today's weather in New Delhi is Mist with a maximum temperature of 34°C and a minimum of 26°C. The current temperature is 28.0°C, but it feels like 30.6°C.\n",
            "\n",
            "=== Testing News Query ===\n",
            "User Input: Give me the latest technology news in India\n",
            "Agent Response: Here are some of the latest technology news headlines in India:\n",
            "* Reliance Jio delays India IPO beyond this year, sources say.\n",
            "* Bitcoin soars to all-time peak just shy of $112,000.\n",
            "* Google will be rolling out ads in AI Overviews in India later this year.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8h5WOlZEYipw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vmUp7SOMYimz"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtdSVsYkYijz"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complex langgraph workflow"
      ],
      "metadata": {
        "id": "kMK4M8n_Yi64"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPMSW8ZNYihP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-J9m-CPAGMdt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDLhPrDqGMbt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard libraries\n",
        "import os\n",
        "import getpass\n",
        "from typing import TypedDict, List, Optional, Literal\n",
        "from pprint import pprint\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Import the LangChain components\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Import Pydantic for structured inputs\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Import ReAct agent creation\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# API keys setup\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Google Gemini API key: \")\n",
        "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key: \")\n",
        "\n",
        "# Model initialization\n",
        "model = init_chat_model(\n",
        "    \"gemini-2.0-flash\",\n",
        "    model_provider=\"google_genai\"\n",
        ")\n",
        "\n",
        "# TavilySearch instance\n",
        "search_instance = TavilySearch(max_results=3)\n",
        "\n",
        "# ===== PYDANTIC SCHEMAS =====\n",
        "class WebSearchInput(BaseModel):\n",
        "    query: str\n",
        "    num_results: int = 2\n",
        "\n",
        "class WeatherSearchInput(BaseModel):\n",
        "    city: str\n",
        "    unit: str = \"Celsius\"\n",
        "\n",
        "class NewsSearchInput(BaseModel):\n",
        "    topic: str\n",
        "    region: str\n",
        "\n",
        "# ===== TOOLS =====\n",
        "@tool\n",
        "def search_web(query: str, num_results: int = 2) -> str:\n",
        "    \"\"\"Search the web with a query and number of results.\"\"\"\n",
        "    search_temp = TavilySearch(max_results=num_results)\n",
        "    result = search_temp.invoke(query)\n",
        "    return str(result)\n",
        "\n",
        "@tool\n",
        "def search_weather(usrdata:WeatherSearchInput) -> str:\n",
        "    \"\"\"Search weather information for a city.\"\"\"\n",
        "    query = f\"current weather in {usrdata.city} in {usrdata.unit}\"\n",
        "    result = search_instance.invoke(query)\n",
        "    return str(result)\n",
        "# def search_weather(city: str, unit: str = \"Celsius\") -> str:\n",
        "#     \"\"\"Search weather information for a city.\"\"\"\n",
        "#     query = f\"current weather in {city} in {unit}\"\n",
        "#     result = search_instance.invoke(query)\n",
        "#     return str(result)\n",
        "\n",
        "@tool\n",
        "def search_news(topic: str, region: str = \"India\") -> str:\n",
        "    \"\"\"Search recent news headlines for a topic in a specific region.\"\"\"\n",
        "    query = f\"latest news about {topic} in {region}\"\n",
        "    result = search_instance.invoke(query)\n",
        "    return str(result)\n",
        "\n",
        "# ===== COMPLEX STATE DEFINITION =====\n",
        "class ComplexAgentState(TypedDict):\n",
        "    # Input\n",
        "    user_input: str\n",
        "\n",
        "    # Processing stages\n",
        "    intent: Optional[str]\n",
        "    entities: Optional[dict]\n",
        "\n",
        "    # Results from different nodes\n",
        "    weather_data: Optional[str]\n",
        "    news_data: Optional[str]\n",
        "    general_search_data: Optional[str]\n",
        "\n",
        "    # Error handling\n",
        "    errors: List[str]\n",
        "    retry_count: int\n",
        "\n",
        "    # Node execution tracking\n",
        "    execution_log: List[dict]\n",
        "\n",
        "    # Final output\n",
        "    final_response: str\n",
        "\n",
        "    # Control flow\n",
        "    next_action: Optional[str]\n",
        "\n",
        "# ===== UTILITY FUNCTIONS =====\n",
        "def log_execution(state: ComplexAgentState, node_name: str, status: str, details: str = \"\"):\n",
        "    \"\"\"Log node execution for debugging\"\"\"\n",
        "    timestamp = datetime.now().isoformat()\n",
        "    log_entry = {\n",
        "        \"timestamp\": timestamp,\n",
        "        \"node\": node_name,\n",
        "        \"status\": status,\n",
        "        \"details\": details\n",
        "    }\n",
        "\n",
        "    if \"execution_log\" not in state:\n",
        "        state[\"execution_log\"] = []\n",
        "\n",
        "    state[\"execution_log\"].append(log_entry)\n",
        "\n",
        "    # Pretty print for real-time monitoring\n",
        "    print(f\"\\n🔄 [{timestamp}] NODE: {node_name} | STATUS: {status}\")\n",
        "    if details:\n",
        "        print(f\"   📝 Details: {details}\")\n",
        "\n",
        "def pretty_print_state(state: ComplexAgentState, title: str):\n",
        "    \"\"\"Pretty print the current state\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"📊 {title}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    for key, value in state.items():\n",
        "        if key == \"execution_log\":\n",
        "            continue  # Skip execution log in state dump\n",
        "\n",
        "        if isinstance(value, str) and len(value) > 100:\n",
        "            print(f\"{key}: {value[:100]}...\")\n",
        "        else:\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "# ===== NODE FUNCTIONS =====\n",
        "\n",
        "def intent_classifier(state: ComplexAgentState) -> ComplexAgentState:\n",
        "    \"\"\"Classify user intent and extract entities\"\"\"\n",
        "    log_execution(state, \"intent_classifier\", \"STARTED\")\n",
        "\n",
        "    try:\n",
        "        user_input = state[\"user_input\"].lower()\n",
        "\n",
        "        # Simple intent classification\n",
        "        if \"weather\" in user_input:\n",
        "            state[\"intent\"] = \"weather\"\n",
        "        elif \"news\" in user_input:\n",
        "            state[\"intent\"] = \"news\"\n",
        "        elif any(word in user_input for word in [\"search\", \"find\", \"look\"]):\n",
        "            state[\"intent\"] = \"search\"\n",
        "        else:\n",
        "            state[\"intent\"] = \"general\"\n",
        "\n",
        "        # Extract entities (simple keyword extraction)\n",
        "        entities = {}\n",
        "\n",
        "        # Extract city names (simple approach)\n",
        "        cities = [\"delhi\", \"mumbai\", \"bangalore\", \"chennai\", \"kolkata\", \"hyderabad\"]\n",
        "        for city in cities:\n",
        "            if city in user_input:\n",
        "                entities[\"city\"] = city.title()\n",
        "                break\n",
        "\n",
        "        # Extract topics\n",
        "        if \"technology\" in user_input or \"tech\" in user_input:\n",
        "            entities[\"topic\"] = \"technology\"\n",
        "        elif \"sports\" in user_input:\n",
        "            entities[\"topic\"] = \"sports\"\n",
        "        elif \"politics\" in user_input:\n",
        "            entities[\"topic\"] = \"politics\"\n",
        "\n",
        "        state[\"entities\"] = entities\n",
        "\n",
        "        log_execution(state, \"intent_classifier\", \"SUCCESS\",\n",
        "                     f\"Intent: {state['intent']}, Entities: {entities}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    except Exception as e:\n",
        "        state[\"errors\"].append(f\"Intent classification failed: {str(e)}\")\n",
        "        log_execution(state, \"intent_classifier\", \"ERROR\", str(e))\n",
        "        return state\n",
        "\n",
        "def weather_node(state: ComplexAgentState) -> ComplexAgentState:\n",
        "    \"\"\"Handle weather-related queries\"\"\"\n",
        "    log_execution(state, \"weather_node\", \"STARTED\")\n",
        "\n",
        "    try:\n",
        "        entities = state.get(\"entities\", {})\n",
        "        city = entities.get(\"city\", \"New Delhi\")  # Default city\n",
        "\n",
        "        # Call the tool directly with parameters\n",
        "        result = search_weather.invoke({\"usrdata\": {\"city\": \"Delhi\", \"unit\": \"Celsius\"}})\n",
        "\n",
        "        state[\"weather_data\"] = result\n",
        "        log_execution(state, \"weather_node\", \"SUCCESS\", f\"Retrieved weather for {city}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    except Exception as e:\n",
        "        state[\"errors\"].append(f\"Weather search failed: {str(e)}\")\n",
        "        log_execution(state, \"weather_node\", \"ERROR\", str(e))\n",
        "        state[\"retry_count\"] = state.get(\"retry_count\", 0) + 1\n",
        "        return state\n",
        "\n",
        "def news_node(state: ComplexAgentState) -> ComplexAgentState:\n",
        "    \"\"\"Handle news-related queries\"\"\"\n",
        "    log_execution(state, \"news_node\", \"STARTED\")\n",
        "\n",
        "    try:\n",
        "        entities = state.get(\"entities\", {})\n",
        "        topic = entities.get(\"topic\", \"general\")\n",
        "        region = entities.get(\"city\", \"India\")\n",
        "\n",
        "        # Call the tool directly with parameters\n",
        "        result = search_news.invoke({\"topic\": topic, \"region\": region})\n",
        "\n",
        "        state[\"news_data\"] = result\n",
        "        log_execution(state, \"news_node\", \"SUCCESS\", f\"Retrieved news for {topic} in {region}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    except Exception as e:\n",
        "        state[\"errors\"].append(f\"News search failed: {str(e)}\")\n",
        "        log_execution(state, \"news_node\", \"ERROR\", str(e))\n",
        "        state[\"retry_count\"] = state.get(\"retry_count\", 0) + 1\n",
        "        return state\n",
        "\n",
        "def general_search_node(state: ComplexAgentState) -> ComplexAgentState:\n",
        "    \"\"\"Handle general search queries\"\"\"\n",
        "    log_execution(state, \"general_search_node\", \"STARTED\")\n",
        "\n",
        "    try:\n",
        "        query = state[\"user_input\"]\n",
        "\n",
        "        # Call the tool directly with parameters\n",
        "        result = search_web.invoke({\"query\": query, \"num_results\": 3})\n",
        "\n",
        "        state[\"general_search_data\"] = result\n",
        "        log_execution(state, \"general_search_node\", \"SUCCESS\", f\"Retrieved search results for: {query}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    except Exception as e:\n",
        "        state[\"errors\"].append(f\"General search failed: {str(e)}\")\n",
        "        log_execution(state, \"general_search_node\", \"ERROR\", str(e))\n",
        "        state[\"retry_count\"] = state.get(\"retry_count\", 0) + 1\n",
        "        return state\n",
        "\n",
        "def error_handler_node(state: ComplexAgentState) -> ComplexAgentState:\n",
        "    \"\"\"Handle errors and decide on retries\"\"\"\n",
        "    log_execution(state, \"error_handler_node\", \"STARTED\")\n",
        "\n",
        "    retry_count = state.get(\"retry_count\", 0)\n",
        "\n",
        "    if retry_count < 2:  # Max 2 retries\n",
        "        log_execution(state, \"error_handler_node\", \"RETRY\", f\"Retry attempt {retry_count + 1}\")\n",
        "        state[\"next_action\"] = \"retry\"\n",
        "    else:\n",
        "        log_execution(state, \"error_handler_node\", \"GIVE_UP\", \"Max retries reached\")\n",
        "        state[\"next_action\"] = \"finalize\"\n",
        "\n",
        "    return state\n",
        "\n",
        "def response_synthesizer(state: ComplexAgentState) -> ComplexAgentState:\n",
        "    \"\"\"Synthesize final response from all collected data\"\"\"\n",
        "    log_execution(state, \"response_synthesizer\", \"STARTED\")\n",
        "\n",
        "    try:\n",
        "        response_parts = []\n",
        "\n",
        "        # Add weather data if available\n",
        "        if state.get(\"weather_data\"):\n",
        "            response_parts.append(f\"🌤️ Weather Information:\\n{state['weather_data']}\")\n",
        "\n",
        "        # Add news data if available\n",
        "        if state.get(\"news_data\"):\n",
        "            response_parts.append(f\"📰 News Information:\\n{state['news_data']}\")\n",
        "\n",
        "        # Add general search data if available\n",
        "        if state.get(\"general_search_data\"):\n",
        "            response_parts.append(f\"🔍 Search Results:\\n{state['general_search_data']}\")\n",
        "\n",
        "        # Handle errors\n",
        "        if state.get(\"errors\"):\n",
        "            response_parts.append(f\"⚠️ Errors encountered:\\n\" + \"\\n\".join(state[\"errors\"]))\n",
        "\n",
        "        # Create final response\n",
        "        if response_parts:\n",
        "            final_response = \"\\n\\n\".join(response_parts)\n",
        "        else:\n",
        "            final_response = \"I apologize, but I couldn't retrieve any information for your query.\"\n",
        "\n",
        "        final_response = model.invoke([{\"role\": \"assistant\", \"content\": final_response},\n",
        "                      {\"role\": \"user\", \"content\": \"Organize this information into a concise pointwise response.\"},\n",
        "                      ])\n",
        "\n",
        "        state[\"final_response\"] = final_response.content\n",
        "        log_execution(state, \"response_synthesizer\", \"SUCCESS\", \"Final response created\")\n",
        "        print(f\"Final Response in synthesizer: {final_response}\")\n",
        "        return state\n",
        "\n",
        "    except Exception as e:\n",
        "        state[\"errors\"].append(f\"Response synthesis failed: {str(e)}\")\n",
        "        log_execution(state, \"response_synthesizer\", \"ERROR\", str(e))\n",
        "        state[\"final_response\"] = \"An error occurred while processing your request.\"\n",
        "        return state\n",
        "\n",
        "# ===== ROUTING FUNCTIONS =====\n",
        "\n",
        "def should_get_weather(state: ComplexAgentState) -> bool:\n",
        "    \"\"\"Check if we should get weather data\"\"\"\n",
        "    return state.get(\"intent\") in [\"weather\", \"general\"] and not state.get(\"weather_data\")\n",
        "\n",
        "def should_get_news(state: ComplexAgentState) -> bool:\n",
        "    \"\"\"Check if we should get news data\"\"\"\n",
        "    return state.get(\"intent\") in [\"news\", \"general\"] and not state.get(\"news_data\")\n",
        "\n",
        "def should_do_general_search(state: ComplexAgentState) -> bool:\n",
        "    \"\"\"Check if we should do general search\"\"\"\n",
        "    return state.get(\"intent\") == \"search\" and not state.get(\"general_search_data\")\n",
        "\n",
        "def should_handle_error(state: ComplexAgentState) -> bool:\n",
        "    \"\"\"Check if we should handle errors\"\"\"\n",
        "    return len(state.get(\"errors\", [])) > 0 and state.get(\"retry_count\", 0) < 2\n",
        "\n",
        "def route_after_classification(state: ComplexAgentState) -> str:\n",
        "    \"\"\"Route after intent classification\"\"\"\n",
        "    if should_handle_error(state):\n",
        "        return \"handle_error\"\n",
        "    elif should_get_weather(state):\n",
        "        return \"weather\"\n",
        "    elif should_get_news(state):\n",
        "        return \"news\"\n",
        "    elif should_do_general_search(state):\n",
        "        return \"general_search\"\n",
        "    else:\n",
        "        return \"synthesizer\"\n",
        "\n",
        "def route_after_data_collection(state: ComplexAgentState) -> str:\n",
        "    \"\"\"Route after data collection nodes\"\"\"\n",
        "    if should_handle_error(state):\n",
        "        return \"handle_error\"\n",
        "    elif should_get_weather(state):\n",
        "        return \"weather\"\n",
        "    elif should_get_news(state):\n",
        "        return \"news\"\n",
        "    elif should_do_general_search(state):\n",
        "        return \"general_search\"\n",
        "    else:\n",
        "        return \"synthesizer\"\n",
        "\n",
        "def route_after_error_handler(state: ComplexAgentState) -> str:\n",
        "    \"\"\"Route after error handling\"\"\"\n",
        "    if state.get(\"next_action\") == \"retry\":\n",
        "        return route_after_classification(state)\n",
        "    else:\n",
        "        return \"synthesizer\"\n",
        "\n",
        "# ===== CREATE COMPLEX WORKFLOW =====\n",
        "\n",
        "def create_complex_workflow():\n",
        "    \"\"\"Create a complex workflow with multiple nodes and error handling\"\"\"\n",
        "\n",
        "    workflow = StateGraph(ComplexAgentState)\n",
        "\n",
        "    # Add all nodes\n",
        "    workflow.add_node(\"classifier\", intent_classifier)\n",
        "    workflow.add_node(\"weather\", weather_node)\n",
        "    workflow.add_node(\"news\", news_node)\n",
        "    workflow.add_node(\"general_search\", general_search_node)\n",
        "    workflow.add_node(\"handle_error\", error_handler_node)\n",
        "    workflow.add_node(\"synthesizer\", response_synthesizer)\n",
        "\n",
        "    # Define edges\n",
        "    workflow.add_edge(START, \"classifier\")\n",
        "\n",
        "    # Conditional edges from classifier\n",
        "    workflow.add_conditional_edges(\n",
        "        \"classifier\",\n",
        "        route_after_classification,\n",
        "        {\n",
        "            \"weather\": \"weather\",\n",
        "            \"news\": \"news\",\n",
        "            \"general_search\": \"general_search\",\n",
        "            \"handle_error\": \"handle_error\",\n",
        "            \"synthesizer\": \"synthesizer\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Conditional edges from data collection nodes\n",
        "    workflow.add_conditional_edges(\n",
        "        \"weather\",\n",
        "        route_after_data_collection,\n",
        "        {\n",
        "            \"weather\": \"weather\",\n",
        "            \"news\": \"news\",\n",
        "            \"general_search\": \"general_search\",\n",
        "            \"handle_error\": \"handle_error\",\n",
        "            \"synthesizer\": \"synthesizer\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"news\",\n",
        "        route_after_data_collection,\n",
        "        {\n",
        "            \"weather\": \"weather\",\n",
        "            \"news\": \"news\",\n",
        "            \"general_search\": \"general_search\",\n",
        "            \"handle_error\": \"handle_error\",\n",
        "            \"synthesizer\": \"synthesizer\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"general_search\",\n",
        "        route_after_data_collection,\n",
        "        {\n",
        "            \"weather\": \"weather\",\n",
        "            \"news\": \"news\",\n",
        "            \"general_search\": \"general_search\",\n",
        "            \"handle_error\": \"handle_error\",\n",
        "            \"synthesizer\": \"synthesizer\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Error handler routing\n",
        "    workflow.add_conditional_edges(\n",
        "        \"handle_error\",\n",
        "        route_after_error_handler,\n",
        "        {\n",
        "            \"weather\": \"weather\",\n",
        "            \"news\": \"news\",\n",
        "            \"general_search\": \"general_search\",\n",
        "            \"synthesizer\": \"synthesizer\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # End at synthesizer\n",
        "    workflow.add_edge(\"synthesizer\", END)\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "# ===== TEST THE COMPLEX WORKFLOW =====\n",
        "\n",
        "def test_complex_workflow():\n",
        "    \"\"\"Test the complex workflow with different queries\"\"\"\n",
        "\n",
        "    # Create the workflow\n",
        "    complex_agent = create_complex_workflow()\n",
        "\n",
        "    # Test cases\n",
        "    test_cases = [\n",
        "        \"What's the weather like in Delhi and give me latest technology news?\",\n",
        "        \"Search for information about latest model of xai, grok and its information\",\n",
        "        \"Give me news related to latest IPOs in india\",\n",
        "    ]\n",
        "\n",
        "    for i, query in enumerate(test_cases, 1):\n",
        "        print(f\"\\n{'🚀 TEST CASE ' + str(i) + ' ':=^60}\")\n",
        "        print(f\"Query: {query}\")\n",
        "\n",
        "        # Create initial state\n",
        "        initial_state = {\n",
        "            \"user_input\": query,\n",
        "            \"intent\": None,\n",
        "            \"entities\": None,\n",
        "            \"weather_data\": None,\n",
        "            \"news_data\": None,\n",
        "            \"general_search_data\": None,\n",
        "            \"errors\": [],\n",
        "            \"retry_count\": 0,\n",
        "            \"execution_log\": [],\n",
        "            \"final_response\": \"\",\n",
        "            \"next_action\": None\n",
        "        }\n",
        "\n",
        "        # Run the workflow\n",
        "        result = complex_agent.invoke(initial_state)\n",
        "\n",
        "        # Show final result\n",
        "        print(f\"\\n{'📋 FINAL RESULT ':=^60}\")\n",
        "        print(result[\"final_response\"])\n",
        "\n",
        "        # # Show execution log\n",
        "        # print(f\"\\n{'📊 EXECUTION LOG ':=^60}\")\n",
        "        # for log_entry in result[\"execution_log\"]:\n",
        "        #     print(f\"[{log_entry['timestamp']}] {log_entry['node']} - {log_entry['status']}\")\n",
        "        #     if log_entry['details']:\n",
        "        #         print(f\"   {log_entry['details']}\")\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "\n",
        "# Run the test\n",
        "if __name__ == \"__main__\":\n",
        "    test_complex_workflow()"
      ],
      "metadata": {
        "id": "cCAY6FTGGMXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15279b65-419a-4ff2-acc1-bf1221aeba84"
      },
      "execution_count": 49,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=======================🚀 TEST CASE 1 =======================\n",
            "Query: What's the weather like in Delhi and give me latest technology news?\n",
            "\n",
            "🔄 [2025-07-10T16:27:00.465618] NODE: intent_classifier | STATUS: STARTED\n",
            "\n",
            "🔄 [2025-07-10T16:27:00.465659] NODE: intent_classifier | STATUS: SUCCESS\n",
            "   📝 Details: Intent: weather, Entities: {'city': 'Delhi', 'topic': 'technology'}\n",
            "\n",
            "🔄 [2025-07-10T16:27:00.466194] NODE: weather_node | STATUS: STARTED\n",
            "\n",
            "🔄 [2025-07-10T16:27:02.316304] NODE: weather_node | STATUS: SUCCESS\n",
            "   📝 Details: Retrieved weather for Delhi\n",
            "\n",
            "🔄 [2025-07-10T16:27:02.317732] NODE: response_synthesizer | STATUS: STARTED\n",
            "\n",
            "🔄 [2025-07-10T16:27:03.579361] NODE: response_synthesizer | STATUS: SUCCESS\n",
            "   📝 Details: Final response created\n",
            "Final Response in synthesizer: content=\"Okay, here's a concise, point-by-point summary of the weather information for Delhi:\\n\\n*   **Location:** The first result refers to Delhi, Ontario, Canada. The other results refer to New Delhi, India.\\n*   **Delhi, Ontario, Canada:** The current temperature is 28.2°C and feels like 31.8°C. The weather is sunny.\\n*   **New Delhi, India:** The weather is expected to be Mist with a maximum temperature of 34°C and a minimum of 26°C.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--d2473daf-12bd-4b0b-9f8d-7a334b464980-0' usage_metadata={'input_tokens': 805, 'output_tokens': 121, 'total_tokens': 926, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "======================📋 FINAL RESULT =======================\n",
            "Okay, here's a concise, point-by-point summary of the weather information for Delhi:\n",
            "\n",
            "*   **Location:** The first result refers to Delhi, Ontario, Canada. The other results refer to New Delhi, India.\n",
            "*   **Delhi, Ontario, Canada:** The current temperature is 28.2°C and feels like 31.8°C. The weather is sunny.\n",
            "*   **New Delhi, India:** The weather is expected to be Mist with a maximum temperature of 34°C and a minimum of 26°C.\n",
            "\n",
            "============================================================\n",
            "\n",
            "=======================🚀 TEST CASE 2 =======================\n",
            "Query: Search for information about latest model of xai, grok and its information\n",
            "\n",
            "🔄 [2025-07-10T16:27:03.581951] NODE: intent_classifier | STATUS: STARTED\n",
            "\n",
            "🔄 [2025-07-10T16:27:03.582008] NODE: intent_classifier | STATUS: SUCCESS\n",
            "   📝 Details: Intent: search, Entities: {}\n",
            "\n",
            "🔄 [2025-07-10T16:27:03.582746] NODE: general_search_node | STATUS: STARTED\n",
            "\n",
            "🔄 [2025-07-10T16:27:05.047774] NODE: general_search_node | STATUS: SUCCESS\n",
            "   📝 Details: Retrieved search results for: Search for information about latest model of xai, grok and its information\n",
            "\n",
            "🔄 [2025-07-10T16:27:05.048935] NODE: response_synthesizer | STATUS: STARTED\n",
            "\n",
            "🔄 [2025-07-10T16:27:06.391878] NODE: response_synthesizer | STATUS: SUCCESS\n",
            "   📝 Details: Final response created\n",
            "Final Response in synthesizer: content='Okay, here\\'s a concise summary of the search results regarding Grok, the AI model from xAI:\\n\\n*   **Grok 3:** xAI has unveiled an early preview of Grok 3, their most advanced model, which combines superior reasoning with extensive pretraining knowledge.\\n*   **Grok\\'s Purpose:** Elon Musk\\'s xAI launched Grok in 2023 as an alternative to other \"woke\" AI chatbots.\\n*   **Grok 4:** Elon Musk unveiled Grok 4, a new version of the AI chatbot for the X platform.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--bae29af1-82ab-4625-9b4c-8eeeaa95691f-0' usage_metadata={'input_tokens': 376, 'output_tokens': 127, 'total_tokens': 503, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "======================📋 FINAL RESULT =======================\n",
            "Okay, here's a concise summary of the search results regarding Grok, the AI model from xAI:\n",
            "\n",
            "*   **Grok 3:** xAI has unveiled an early preview of Grok 3, their most advanced model, which combines superior reasoning with extensive pretraining knowledge.\n",
            "*   **Grok's Purpose:** Elon Musk's xAI launched Grok in 2023 as an alternative to other \"woke\" AI chatbots.\n",
            "*   **Grok 4:** Elon Musk unveiled Grok 4, a new version of the AI chatbot for the X platform.\n",
            "\n",
            "============================================================\n",
            "\n",
            "=======================🚀 TEST CASE 3 =======================\n",
            "Query: Give me news related to latest IPOs in india\n",
            "\n",
            "🔄 [2025-07-10T16:27:06.394954] NODE: intent_classifier | STATUS: STARTED\n",
            "\n",
            "🔄 [2025-07-10T16:27:06.394996] NODE: intent_classifier | STATUS: SUCCESS\n",
            "   📝 Details: Intent: news, Entities: {}\n",
            "\n",
            "🔄 [2025-07-10T16:27:06.395790] NODE: news_node | STATUS: STARTED\n",
            "\n",
            "🔄 [2025-07-10T16:27:07.623963] NODE: news_node | STATUS: SUCCESS\n",
            "   📝 Details: Retrieved news for general in India\n",
            "\n",
            "🔄 [2025-07-10T16:27:07.625139] NODE: response_synthesizer | STATUS: STARTED\n",
            "\n",
            "🔄 [2025-07-10T16:27:09.183710] NODE: response_synthesizer | STATUS: SUCCESS\n",
            "   📝 Details: Final response created\n",
            "Final Response in synthesizer: content=\"Here's a concise summary of the news information:\\n\\n*   **Indian Army Agniveer Recruitment:** The Indian Army started the Agniveer 2025 recruitment exams on June 30, 2025.\\n*   **General Admits Losses:** General Anil Chauhan admitted that India experienced aerial losses in a recent conflict with Pakistan.\\n*   **India News:** An article mentions a driver who was intoxicated took his car into Tejashwi Yadav's motorcade.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--f3ec13f5-9b8b-4139-bd4f-89aa7b2d3370-0' usage_metadata={'input_tokens': 362, 'output_tokens': 101, 'total_tokens': 463, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "======================📋 FINAL RESULT =======================\n",
            "Here's a concise summary of the news information:\n",
            "\n",
            "*   **Indian Army Agniveer Recruitment:** The Indian Army started the Agniveer 2025 recruitment exams on June 30, 2025.\n",
            "*   **General Admits Losses:** General Anil Chauhan admitted that India experienced aerial losses in a recent conflict with Pakistan.\n",
            "*   **India News:** An article mentions a driver who was intoxicated took his car into Tejashwi Yadav's motorcade.\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}