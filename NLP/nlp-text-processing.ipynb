{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f1d7e2",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a79deb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data path: ['D:\\\\py_prac\\\\langchain-prac\\\\nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to D:\\py_prac\\langchain-\n",
      "[nltk_data]     prac\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to D:\\py_prac\\langchain-\n",
      "[nltk_data]     prac\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "# Set NLTK data path first\n",
    "nltk.data.path.clear()\n",
    "nltk.data.path.append(r'D:\\py_prac\\langchain-prac\\nltk_data')\n",
    "\n",
    "# Download required resources to the correct directory\n",
    "# Note: Updated to use punkt_tab instead of punkt\n",
    "nltk.download('punkt_tab', download_dir=r'D:\\py_prac\\langchain-prac\\nltk_data')\n",
    "nltk.download('stopwords', download_dir=r'D:\\py_prac\\langchain-prac\\nltk_data')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Test the setup\n",
    "print(\"NLTK data path:\", nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4629e306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'horses are beautiful animals. they run fast and are very strong.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Horses are beautiful animals. They run fast and are very strong.\"\n",
    "\n",
    "\n",
    "## STEP 1: Lowercase the text\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a1cec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['horses',\n",
       " 'are',\n",
       " 'beautiful',\n",
       " 'animals',\n",
       " '.',\n",
       " 'they',\n",
       " 'run',\n",
       " 'fast',\n",
       " 'and',\n",
       " 'are',\n",
       " 'very',\n",
       " 'strong',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## STEEP 2: Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8058251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1ad9ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without punctuation: ['horses', 'are', 'beautiful', 'animals', 'they', 'run', 'fast', 'and', 'are', 'very', 'strong']\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Remove punctuation\n",
    "tokens_no_punct = [token for token in tokens if token not in string.punctuation]\n",
    "print(\"Tokens without punctuation:\", tokens_no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d467aff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop_words\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered tokens (no stopwords): ['horses', 'beautiful', 'animals', 'run', 'fast', 'strong']\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens_no_punct if token not in stop_words]\n",
    "print(\"Filtered tokens (no stopwords):\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d1681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed tokens: ['hors', 'beauti', 'anim', 'run', 'fast', 'strong']\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"Stemmed tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287212f8",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b40bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['bark' 'cats' 'chase' 'dogs' 'loudly' 'mice']\n",
      "Bag of Words Matrix:\n",
      " [[0 1 1 0 0 1]\n",
      " [0 1 1 0 0 1]\n",
      " [1 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Example: Bag of Words with scikit-learn (latest version)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"Cats chase mice.\",\n",
    "    \"Mice chase cats.\",\n",
    "    \"Dogs bark loudly.\"\n",
    "]\n",
    "\n",
    "# Create the vectorizer and fit_transform the documents\n",
    "vectorizer = CountVectorizer(max_features=100, stop_words='english', lowercase=True)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Show the feature names (vocabulary)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Show the Bag of Words matrix\n",
    "print(\"Bag of Words Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567533d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d9410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414808d",
   "metadata": {},
   "source": [
    "## TF-IDF  - Term frequency - inv. document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c482dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'are' 'cats' 'chase' 'dogs' 'mice' 'pets']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.42544054 0.54783215 0.         0.72033345\n",
      "  0.        ]\n",
      " [0.         0.         0.48133417 0.61980538 0.61980538 0.\n",
      "  0.        ]\n",
      " [0.50461134 0.50461134 0.29803159 0.         0.38376993 0.\n",
      "  0.50461134]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"Cats chase mice.\",\n",
    "    \"Dogs chase cats.\",\n",
    "    \"Cats and dogs are pets.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be235446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\pyinstallfolder\\py312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in d:\\pyinstallfolder\\py312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\pyinstallfolder\\py312\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\pyinstallfolder\\py312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\pyinstallfolder\\py312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\pyinstallfolder\\py312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1512cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e992212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "316ae36b",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f664b816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Creating blank language object then\n",
    "# tokenizing words of the sentence\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Dynstat github is a one stop\\\n",
    "learning destination for geeks.\")\n",
    "\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17530b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98d36c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_iter = iter(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b01ea0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeeksforGeeks"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(doc_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "401fc382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_context',\n",
       " '_get_array_attrs',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'copy',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_dict',\n",
       " 'from_disk',\n",
       " 'from_docs',\n",
       " 'from_json',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_annotation',\n",
       " 'has_extension',\n",
       " 'has_unknown_spaces',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_ents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'spans',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_dict',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'to_utf8_array',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee013646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeeksforGeeks\n",
      "is\n",
      "a\n",
      "one\n",
      "stoplearning\n",
      "destination\n",
      "for\n",
      "geeks\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18e171d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 5.6 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 4.6 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.6/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.2/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 4.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.8/12.8 MB 4.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 4.1 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 4.0 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 4.0 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "689a9688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51446b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tok2vec': [],\n",
       " 'tagger': ['$',\n",
       "  \"''\",\n",
       "  ',',\n",
       "  '-LRB-',\n",
       "  '-RRB-',\n",
       "  '.',\n",
       "  ':',\n",
       "  'ADD',\n",
       "  'AFX',\n",
       "  'CC',\n",
       "  'CD',\n",
       "  'DT',\n",
       "  'EX',\n",
       "  'FW',\n",
       "  'HYPH',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'JJR',\n",
       "  'JJS',\n",
       "  'LS',\n",
       "  'MD',\n",
       "  'NFP',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'NNPS',\n",
       "  'NNS',\n",
       "  'PDT',\n",
       "  'POS',\n",
       "  'PRP',\n",
       "  'PRP$',\n",
       "  'RB',\n",
       "  'RBR',\n",
       "  'RBS',\n",
       "  'RP',\n",
       "  'SYM',\n",
       "  'TO',\n",
       "  'UH',\n",
       "  'VB',\n",
       "  'VBD',\n",
       "  'VBG',\n",
       "  'VBN',\n",
       "  'VBP',\n",
       "  'VBZ',\n",
       "  'WDT',\n",
       "  'WP',\n",
       "  'WP$',\n",
       "  'WRB',\n",
       "  'XX',\n",
       "  '_SP',\n",
       "  '``'],\n",
       " 'parser': ['ROOT',\n",
       "  'acl',\n",
       "  'acomp',\n",
       "  'advcl',\n",
       "  'advmod',\n",
       "  'agent',\n",
       "  'amod',\n",
       "  'appos',\n",
       "  'attr',\n",
       "  'aux',\n",
       "  'auxpass',\n",
       "  'case',\n",
       "  'cc',\n",
       "  'ccomp',\n",
       "  'compound',\n",
       "  'conj',\n",
       "  'csubj',\n",
       "  'csubjpass',\n",
       "  'dative',\n",
       "  'dep',\n",
       "  'det',\n",
       "  'dobj',\n",
       "  'expl',\n",
       "  'intj',\n",
       "  'mark',\n",
       "  'meta',\n",
       "  'neg',\n",
       "  'nmod',\n",
       "  'npadvmod',\n",
       "  'nsubj',\n",
       "  'nsubjpass',\n",
       "  'nummod',\n",
       "  'oprd',\n",
       "  'parataxis',\n",
       "  'pcomp',\n",
       "  'pobj',\n",
       "  'poss',\n",
       "  'preconj',\n",
       "  'predet',\n",
       "  'prep',\n",
       "  'prt',\n",
       "  'punct',\n",
       "  'quantmod',\n",
       "  'relcl',\n",
       "  'xcomp'],\n",
       " 'attribute_ruler': [],\n",
       " 'lemmatizer': [],\n",
       " 'ner': ['CARDINAL',\n",
       "  'DATE',\n",
       "  'EVENT',\n",
       "  'FAC',\n",
       "  'GPE',\n",
       "  'LANGUAGE',\n",
       "  'LAW',\n",
       "  'LOC',\n",
       "  'MONEY',\n",
       "  'NORP',\n",
       "  'ORDINAL',\n",
       "  'ORG',\n",
       "  'PERCENT',\n",
       "  'PERSON',\n",
       "  'PRODUCT',\n",
       "  'QUANTITY',\n",
       "  'TIME',\n",
       "  'WORK_OF_ART']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20550b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising doc with a sentence.\n",
    "doc = nlp(\"If you want to be an excellent programmer \\\n",
    ", be consistent practicing daily on leetcode.\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If | subordinating conjunction | if\n",
      "you | pronoun | you\n",
      "want | verb | want\n",
      "to | particle | to\n",
      "be | auxiliary | be\n",
      "an | determiner | an\n",
      "excellent | adjective | excellent\n",
      "programmer | noun | programmer\n",
      ", | punctuation | ,\n",
      "be | auxiliary | be\n",
      "consistent | adjective | consistent\n",
      "practicing | verb | practice\n",
      "daily | adverb | daily\n",
      "on | adposition | on\n",
      "leetcode | proper noun | leetcode\n",
      ". | punctuation | .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token} | {spacy.explain(token.pos_)} | {token.lemma_}\")\n",
    "\n",
    "# notice that the lemma of the word \"practicing\" is \"practice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0f34ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCONJ\n",
      "PRON\n",
      "VERB\n",
      "PART\n",
      "AUX\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n",
      "AUX\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "323449e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if\n",
      "you\n",
      "want\n",
      "to\n",
      "be\n",
      "an\n",
      "excellent\n",
      "programmer\n",
      ",\n",
      "be\n",
      "consistent\n",
      "practice\n",
      "daily\n",
      "on\n",
      "leetcode\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d288dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd810f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3fd6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
